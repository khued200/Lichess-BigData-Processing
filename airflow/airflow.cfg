[core]
# The home folder for airflow, default is ~/airflow
airflow_home = /opt/airflow/

# The folder where your airflow dags are stored. You can use this value to change your DAG folder.
dags_folder = /opt/airflow//dags

# The folder where airflow's logs are stored
base_log_folder = /opt/airflow//logs

# The default timezone for airflow
default_timezone = utc

# The executor to use. Options are 'SequentialExecutor', 'LocalExecutor', 'CeleryExecutor', 'DaskExecutor'.
executor = SequentialExecutor

# If you have sensitive values such as passwords or API tokens, you can enable the secrets backend to retrieve them
# from your backend (e.g., AWS Secrets Manager, HashiCorp Vault, etc.).
# Example:
# secrets_backend = airflow.providers.amazon.aws.secrets.secrets_manager.SecretsManagerBackend

[logging]
# The log level to capture (DEBUG, INFO, WARNING, ERROR, CRITICAL)
logging_level = INFO

# The format for logging output
# The log filename format
log_filename_template = airflow/logs/{dag_id}/{task_id}/{execution_date}/{try_number}.log

# How many log files to retain per task instance (default is 10)
max_log_file_size = 5MB

# Rotate logs every n bytes
log_rotation_size = 50000000

[database]
# The SQLAlchemy connection string for your backend database
# Example for PostgreSQL
# sql_alchemy_conn = postgresql+psycopg2://username:password@hostname:port/database

sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@postgres:5432/airflow

# The maximum number of database connections in the pool
sql_alchemy_pool_size = 5

# The pool recycle time
sql_alchemy_pool_recycle = 1800

[webserver]
# The web server port (default 8080)
web_server_port = 8080

# Enable/disable authentication on the web UI
web_server_authentication = True

# Enable/disable the web server. If set to False, the web UI won't be accessible.
web_server = True

# The base URL for the web interface
web_base_url = http://localhost:8080

[celery]
# Celery settings for running tasks asynchronously
# Broker to use for communication between workers and the scheduler
broker_url = redis://localhost:6379/0

# Result backend to store task results (if applicable)
result_backend = db+postgresql://airflow:airflow@localhost:5432/airflow

# Number of celery workers to spawn
worker_concurrency = 4

[secrets]
# Configuration for retrieving secrets from backend systems (e.g., AWS Secrets Manager, Vault)
# secrets_backend = airflow.providers.amazon.aws.secrets.secrets_manager.SecretsManagerBackend

# [scheduler]
# # How often the scheduler checks for new DAGs (in seconds)
# scheduler_interval = 60